name: Python Scraper
on:
  push:
    branches:
      - main
  schedule:
    - cron:  '0 */12 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
     - name: Checkout branch gh-pages
       uses: actions/checkout@v1
       with:
        fetch-depth: 0
        ref: 'gh-pages'
     - name: Checkout branch main
       uses: actions/checkout@v1
       with:
        fetch-depth: 0
        ref: 'main'
        clean: false
     - name: Setup Python 3
       uses: actions/setup-python@v2
       with:
        python-version: '3.x'
        architecture: 'x64' 
     - name: Cache deps
       uses: actions/cache@v2
       with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          ${{ runner.os }}-
     - name: Install deps
       run: pip install -r requirements.txt
     - name: Load historical data
       run: git restore -s gh-pages -SW -- data.json
     - name: Scrape
       run: python scrape.py
     - name: Assemble deployment
       env:
        GOOGLE_ANALYTICS_KEY: ${{ secrets.GOOGLE_ANALYTICS_KEY }}
       run: |
        rm -rf .github
        rm scrape.py
        rm requirements.txt
        printf "google_analytics: %s" $GOOGLE_ANALYTICS_KEY >> _config.yml
     - name: Commit changes
       run: |
        git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add .
        git commit -m "Updated data"
     - name: Push to prod
       uses: ad-m/github-push-action@master
       with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        branch: gh-pages
        force: true
